{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL52qS43d9_w"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZLLgskSCDVW",
        "outputId": "2f470d02-7517-4965-906b-ff976afad127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download data from here: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv\n",
        "# upload the files to gdrive and uncomment this block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfszfsB5ClOp"
      },
      "outputs": [],
      "source": [
        "DIR = \"/content/drive/MyDrive/725DATA/\"\n",
        "MNIST_TRAIN = DIR + \"mnist_train.csv\"\n",
        "MNIST_TEST = DIR + \"mnist_test.csv\"\n",
        "df = pd.read_csv(MNIST_TRAIN)\n",
        "df.head()\n",
        "data = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xcdmC1KKGKq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki-US5__E2JZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhw_HinBuEWt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from hashlib import sha256\n",
        "\n",
        "def sizeOfShard(container, shard):\n",
        "    '''\n",
        "    Returns the size (in number of points) of the shard before any unlearning request.\n",
        "    '''\n",
        "    shards = np.load(getContainerColabRoot(container) + '/splitfile.npy', allow_pickle=True)\n",
        "    shard = np.int64(shard)\n",
        "\n",
        "    return shards[shard].shape[0]\n",
        "\n",
        "def realSizeOfShard(container, label, shard):\n",
        "    '''\n",
        "    Returns the actual size of the shard (including unlearning requests).\n",
        "    '''\n",
        "    shards = np.load(getContainerColabRoot(container) + '/splitfile.npy', allow_pickle=True)\n",
        "    requests = np.load(getContainerColabRoot(container) + '/requestfile:{}.npy'.format(label), allow_pickle=True)\n",
        "\n",
        "    return shards[shard].shape[0] - requests[shard].shape[0]\n",
        "\n",
        "def getShardHash(container, label, shard, until=None):\n",
        "    '''\n",
        "    Returns a hash of the indices of the points in the shard lower than until\n",
        "    that are not in the requests (separated by :).\n",
        "    '''\n",
        "    shards = np.load(getContainerColabRoot(container) + '/splitfile.npy', allow_pickle=True)\n",
        "    requests = np.load(getContainerColabRoot(container) + '/requestfile:{}.npy'.format(label), allow_pickle=True)\n",
        "\n",
        "    if until == None:\n",
        "        until = shards[shard].shape[0]\n",
        "    indices = np.setdiff1d(shards[shard][:until], requests[shard])\n",
        "    string_of_indices = ':'.join(indices.astype(str))\n",
        "    return sha256(string_of_indices.encode()).hexdigest()\n",
        "\n",
        "def fetchShardBatch(container, label, shard, batch_size, dataset, offset=0, until=None):\n",
        "    '''\n",
        "    Generator returning batches of points in the shard that are not in the requests\n",
        "    with specified batch_size from the specified dataset\n",
        "    optionnally located between offset and until (slicing).\n",
        "    '''\n",
        "    shards = np.load(getContainerColabRoot(container) + '/splitfile.npy', allow_pickle=True)\n",
        "    requests = np.load(getContainerColabRoot(container) + '/requestfile:{}.npy'.format(label), allow_pickle=True)\n",
        "\n",
        "    datasetfile = dataset\n",
        "    dataloader = datasetfile['dataloader']\n",
        "    if until == None or until > shards[shard].shape[0]:\n",
        "        until = shards[shard].shape[0]\n",
        "\n",
        "    limit = offset\n",
        "    while limit <= until - batch_size:\n",
        "        limit += batch_size\n",
        "        indices = np.setdiff1d(shards[shard][limit-batch_size:limit], requests[shard])\n",
        "        l = dataloader(indices)\n",
        "        yield l\n",
        "    if limit < until:\n",
        "        indices = np.setdiff1d(shards[shard][limit:until], requests[shard])\n",
        "        l = dataloader(indices)\n",
        "        yield l\n",
        "\n",
        "def fetchTestBatch(dataset, batch_size):\n",
        "    '''\n",
        "    Generator returning batches of points from the specified test dataset\n",
        "    with specified batch_size.\n",
        "    '''\n",
        "    datasetfile = dataset\n",
        "    dataloader = datasetfile['dataloader']\n",
        "    limit = 0\n",
        "    while limit <= datasetfile['nb_test'] - batch_size:\n",
        "        limit += batch_size\n",
        "        yield dataloader(np.arange(limit - batch_size, limit), category='test')\n",
        "    if limit < datasetfile['nb_test']:\n",
        "        yield dataloader(np.arange(limit, datasetfile['nb_test']), category='test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05uGhZ_l6k_9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.nn.functional import one_hot\n",
        "import os\n",
        "from glob import glob\n",
        "from time import time\n",
        "\n",
        "\n",
        "def getCache(container):\n",
        "    t = \"/content/containers/{}/cache\".format(container)\n",
        "    return t\n",
        "def getTime(container):\n",
        "    t = \"/content/containers/{}/times\".format(container)\n",
        "    return t\n",
        "\n",
        "def getOutputs(container):\n",
        "    t = \"/content/containers/{}/outputs\".format(container)\n",
        "    return t\n",
        "def getContainerColabRoot(container):\n",
        "    t = \"/content/containers/{}\".format(container)\n",
        "    return t\n",
        "\n",
        "class SisaArgs:\n",
        "    model= None\n",
        "    train= False\n",
        "    test= False\n",
        "    epochs= 20\n",
        "    batch_size= 16\n",
        "    dropout_rate= 0.4\n",
        "    learning_rate= 0.001\n",
        "    optimizer= \"adam\"\n",
        "    output_type= \"argmax\"\n",
        "    container= \"\"\n",
        "    shard= \"\"\n",
        "    slices= 1\n",
        "    dataset= {}\n",
        "    chkpt_interval= 1\n",
        "    label= \"latest\"\n",
        "\n",
        "    def __init__(self, model, dataset) -> None:\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "\n",
        "def sisa(args):\n",
        "    # Retrive dataset metadata.\n",
        "    input_shape = tuple(args.dataset[\"input_shape\"])\n",
        "    nb_classes = args.dataset[\"nb_classes\"]\n",
        "\n",
        "    # Use GPU if available.\n",
        "    device = torch.device(\n",
        "        \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )  # pylint: disable=no-member\n",
        "\n",
        "    # Instantiate model and send to selected device.\n",
        "    # model = model_lib.Model(input_shape, nb_classes, dropout_rate=args.dropout_rate)\n",
        "    model = args.model(input_shape, nb_classes, dropout_rate=args.dropout_rate)\n",
        "    model.to(device)\n",
        "\n",
        "    # Instantiate loss and optimizer.\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "    if args.optimizer == \"adam\":\n",
        "        optimizer = Adam(model.parameters(), lr=args.learning_rate)\n",
        "    elif args.optimizer == \"sgd\":\n",
        "        optimizer = SGD(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "    if args.train:\n",
        "        shard_size = sizeOfShard(args.container, args.shard)\n",
        "        slice_size = shard_size // args.slices\n",
        "        avg_epochs_per_slice = (\n",
        "            2 * args.slices / (args.slices + 1) * args.epochs / args.slices\n",
        "        )\n",
        "        loaded = False\n",
        "\n",
        "        for sl in range(args.slices):\n",
        "            # Get slice hash using sharded lib.\n",
        "            slice_hash = getShardHash(\n",
        "                args.container, args.label, args.shard, until=(sl + 1) * slice_size\n",
        "            )\n",
        "\n",
        "            # If checkpoints exists, skip the slice.\n",
        "            if not os.path.exists(\n",
        "                getCache(args.container) + \"/{}.pt\".format(slice_hash)\n",
        "            ):\n",
        "                # Initialize state.\n",
        "                elapsed_time = 0\n",
        "                start_epoch = 0\n",
        "                slice_epochs = int((sl + 1) * avg_epochs_per_slice) - int(\n",
        "                    sl * avg_epochs_per_slice\n",
        "                )\n",
        "\n",
        "                # If weights are already in memory (from previous slice), skip loading.\n",
        "                if not loaded:\n",
        "                    # Look for a recovery checkpoint for the slice.\n",
        "                    recovery_list = glob(\n",
        "                        getCache(args.container) + \"/{}_*.pt\".format(slice_hash)\n",
        "                    )\n",
        "                    if len(recovery_list) > 0:\n",
        "                        print(\n",
        "                            \"Recovery mode for shard {} on slice {}\".format(args.shard, sl)\n",
        "                        )\n",
        "\n",
        "                        # Load weights.\n",
        "                        model.load_state_dict(torch.load(recovery_list[0]))\n",
        "                        start_epoch = int(\n",
        "                            recovery_list[0].split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n",
        "                        )\n",
        "\n",
        "                        # Load time\n",
        "                        with open(\n",
        "                            getTime(args.container) + \"/{}_{}.time\".format(\n",
        "                                 slice_hash, start_epoch\n",
        "                            ),\n",
        "                            \"r\",\n",
        "                        ) as f:\n",
        "                            elapsed_time = float(f.read())\n",
        "\n",
        "                    # If there is no recovery checkpoint and this slice is not the first, load previous slice.\n",
        "                    elif sl > 0:\n",
        "                        previous_slice_hash = getShardHash(\n",
        "                            args.container, args.label, args.shard, until=sl * slice_size\n",
        "                        )\n",
        "\n",
        "                        # Load weights.\n",
        "                        model.load_state_dict(\n",
        "                            torch.load(\n",
        "                                getCache(args.container) + \"/{}.pt\".format(\n",
        "                                    previous_slice_hash\n",
        "                                )\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    # Mark model as loaded for next slices.\n",
        "                    loaded = True\n",
        "\n",
        "                # If this is the first slice, no need to load anything.\n",
        "                elif sl == 0:\n",
        "                    loaded = True\n",
        "\n",
        "                # Actual training.\n",
        "                train_time = 0.0\n",
        "\n",
        "                for epoch in range(start_epoch, slice_epochs):\n",
        "                    epoch_start_time = time()\n",
        "\n",
        "                    for images, labels in fetchShardBatch(\n",
        "                        args.container,\n",
        "                        args.label,\n",
        "                        args.shard,\n",
        "                        args.batch_size,\n",
        "                        args.dataset,\n",
        "                        until=(sl + 1) * slice_size if sl < args.slices - 1 else None,\n",
        "                    ):\n",
        "\n",
        "                        # Convert data to torch format and send to selected device.\n",
        "                        gpu_images = torch.from_numpy(images).to(\n",
        "                            device\n",
        "                        )  # pylint: disable=no-member\n",
        "                        gpu_labels = torch.from_numpy(labels).to(\n",
        "                            device\n",
        "                        )  # pylint: disable=no-member\n",
        "\n",
        "                        forward_start_time = time()\n",
        "\n",
        "                        # Perform basic training step.\n",
        "                        logits = model(gpu_images)\n",
        "                        loss = loss_fn(logits, gpu_labels)\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "\n",
        "                        optimizer.step()\n",
        "\n",
        "                        train_time += time() - forward_start_time\n",
        "\n",
        "                    # Create a checkpoint every chkpt_interval.\n",
        "                    if (\n",
        "                        args.chkpt_interval != -1\n",
        "                        and epoch % args.chkpt_interval == args.chkpt_interval - 1\n",
        "                    ):\n",
        "                        # Save weights\n",
        "                        torch.save(\n",
        "                            model.state_dict(),\n",
        "                            getCache(args.container) + \"/{}_{}.pt\".format(\n",
        "                                slice_hash, epoch\n",
        "                            ),\n",
        "                        )\n",
        "\n",
        "                        # Save time\n",
        "                        with open(\n",
        "                            getTime(args.container) + \"/{}_{}.time\".format(\n",
        "                                 slice_hash, epoch\n",
        "                            ),\n",
        "                            \"w\",\n",
        "                        ) as f:\n",
        "                            f.write(\"{}\\n\".format(train_time + elapsed_time))\n",
        "\n",
        "                        # Remove previous checkpoint.\n",
        "                        if os.path.exists(\n",
        "                            getCache(args.container) + \"/{}_{}.pt\".format(\n",
        "                                slice_hash, epoch - args.chkpt_interval\n",
        "                            )\n",
        "                        ):\n",
        "                            os.remove(\n",
        "                                getCache(args.container) + \"/{}_{}.pt\".format(\n",
        "                                    slice_hash, epoch - args.chkpt_interval\n",
        "                                )\n",
        "                            )\n",
        "                        if os.path.exists(\n",
        "                            getTime(args.container) + \"/{}_{}.time\".format(\n",
        "                                 slice_hash, epoch - args.chkpt_interval\n",
        "                            )\n",
        "                        ):\n",
        "                            os.remove(\n",
        "                                getTime(args.container) + \"/{}_{}.time\".format(\n",
        "                                     slice_hash, epoch - args.chkpt_interval\n",
        "                                )\n",
        "                            )\n",
        "\n",
        "                # When training is complete, save slice.\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    getCache(args.container) + \"/{}.pt\".format(slice_hash),\n",
        "                )\n",
        "                with open(\n",
        "                    getTime(args.container) + \"/{}.time\".format( slice_hash), \"w\"\n",
        "                ) as f:\n",
        "                    f.write(\"{}\\n\".format(train_time + elapsed_time))\n",
        "\n",
        "                # Remove previous checkpoint.\n",
        "                if os.path.exists(\n",
        "                    getCache(args.container) + \"/{}_{}.pt\".format(\n",
        "                        slice_hash, args.epochs - args.chkpt_interval\n",
        "                    )\n",
        "                ):\n",
        "                    os.remove(\n",
        "                        getCache(args.container) + \"/{}_{}.pt\".format(\n",
        "                            slice_hash, args.epochs - args.chkpt_interval\n",
        "                        )\n",
        "                    )\n",
        "                if os.path.exists(\n",
        "                    getTime(args.container) + \"/{}_{}.time\".format(\n",
        "                         slice_hash, args.epochs - args.chkpt_interval\n",
        "                    )\n",
        "                ):\n",
        "                    os.remove(\n",
        "                        getTime(args.container) + \"/{}_{}.time\".format(\n",
        "                             slice_hash, args.epochs - args.chkpt_interval\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # If this is the last slice, create a symlink attached to it.\n",
        "                if sl == args.slices - 1:\n",
        "                    os.symlink(\n",
        "                        \"{}.pt\".format(slice_hash),\n",
        "                        getCache(args.container) + \"/shard-{}:{}.pt\".format(\n",
        "                            args.shard, args.label\n",
        "                        ),\n",
        "                    )\n",
        "                    os.symlink(\n",
        "                        \"{}.time\".format(slice_hash),\n",
        "                        getTime(args.container) + \"/shard-{}:{}.time\".format(\n",
        "                             args.shard, args.label\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "            elif sl == args.slices - 1:\n",
        "                os.symlink(\n",
        "                    \"{}.pt\".format(slice_hash),\n",
        "                    getCache(args.container) + \"/shard-{}:{}.pt\".format(\n",
        "                        args.shard, args.label\n",
        "                    ),\n",
        "                )\n",
        "                if not os.path.exists(\n",
        "                    getTime(args.container) + \"/shard-{}:{}.time\".format(\n",
        "                         args.shard, args.label\n",
        "                    )\n",
        "                ):\n",
        "                    os.symlink(\n",
        "                        \"null.time\",\n",
        "                        getTime(args.container) + \"/shard-{}:{}.time\".format(\n",
        "                             args.shard, args.label\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "\n",
        "    if args.test:\n",
        "        # Load model weights from shard checkpoint (last slice).\n",
        "        model.load_state_dict(\n",
        "            torch.load(\n",
        "                getCache(args.container) + \"/shard-{}:{}.pt\".format(\n",
        "                    args.shard, args.label\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Compute predictions batch per batch.\n",
        "        outputs = np.empty((0, nb_classes))\n",
        "        for images, _ in fetchTestBatch(args.dataset, args.batch_size):\n",
        "            # Convert data to torch format and send to selected device.\n",
        "            gpu_images = torch.from_numpy(images).to(device)  # pylint: disable=no-member\n",
        "\n",
        "            if args.output_type == \"softmax\":\n",
        "                # Actual batch prediction.\n",
        "                logits = model(gpu_images)\n",
        "                predictions = softmax(logits, dim=1).to(\"cpu\")  # Send back to cpu.\n",
        "\n",
        "                # Convert back to numpy and concatenate with previous batches.\n",
        "                outputs = np.concatenate((outputs, predictions.numpy()))\n",
        "\n",
        "            else:\n",
        "                # Actual batch prediction.\n",
        "                logits = model(gpu_images)\n",
        "                predictions = torch.argmax(logits, dim=1)  # pylint: disable=no-member\n",
        "\n",
        "                # Convert to one hot, send back to cpu, convert back to numpy and concatenate with previous batches.\n",
        "                out = one_hot(predictions, nb_classes).to(\"cpu\")\n",
        "                outputs = np.concatenate((outputs, out.numpy()))\n",
        "\n",
        "        # Save outputs in numpy format.\n",
        "        outputs = np.array(outputs)\n",
        "        np.save(\n",
        "            getOutputs(args.container) + \"/shard-{}:{}.npy\".format(\n",
        "                args.shard, args.label\n",
        "            ),\n",
        "            outputs,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKoAXL3cwjCi"
      },
      "outputs": [],
      "source": [
        "traindf = pd.read_csv(MNIST_TRAIN)\n",
        "testdf = pd.read_csv(MNIST_TEST)\n",
        "# traindf[\"label\"].to_numpy()\n",
        "xTraindf = traindf.drop(\"label\",  axis=1)\n",
        "yTraindf = traindf[\"label\"]\n",
        "xTestdf = testdf.drop(\"label\",  axis=1)\n",
        "yTestdf = testdf[\"label\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo7eGIZUNb1w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61U1OyMfvh3E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = xTraindf.to_numpy().astype(np.float32)\n",
        "X_test = xTestdf.to_numpy().astype(np.float32)\n",
        "y_train = yTraindf.to_numpy().astype(np.int64)\n",
        "y_test = yTestdf.to_numpy().astype(np.int64)\n",
        "\n",
        "def load(indices, category='train'):\n",
        "  if category == 'train':\n",
        "      return X_train[indices], y_train[indices]\n",
        "  elif category == 'test':\n",
        "      return X_test[indices], y_test[indices]\n",
        "\n",
        "dataloader = load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CucArDqx2Gdp"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_shape, nb_classes, *args, **kwargs):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_shape[0], 128)\n",
        "        self.fc2 = nn.Linear(128, nb_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.tanh(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ModelCNN_MNIST(nn.Module):\n",
        "    def __init__(self, input_shape, nb_classes, *args, **kwargs):\n",
        "        super(ModelCNN_MNIST, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(3*3*64, 256)\n",
        "        self.fc2 = nn.Linear(256, nb_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1,28,28).float()\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = x.view(-1,3*3*64 )\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFHyrGtWFUIO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os.path\n",
        "from os import path\n",
        "shards = 10\n",
        "NUM_REQUESTS = 25\n",
        "cont = '/content/containers'\n",
        "sh = cont + \"/\" + str(shards)\n",
        "c = sh + \"/\" + \"cache\"\n",
        "o = sh + \"/\" + \"outputs\"\n",
        "\n",
        "def make(dirs):\n",
        "  for dir in dirs:\n",
        "    if path.exists(dir) == False:\n",
        "      os.mkdir(dir)\n",
        "\n",
        "def makeTimeDir():\n",
        "  dir = sh + \"/\" + \"times\"\n",
        "  if path.exists(dir) == False:\n",
        "    !echo 0 > \"containers/5/times/null.time\"\n",
        "    os.mkdir(dir)\n",
        "make([cont, sh, c, o])\n",
        "makeTimeDir()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1YhWjxYiEPs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import load_npz\n",
        "# to_numpy().astype(np.float32)\n",
        "\n",
        "def KmeansCluster(trainDF, num_class = 10):\n",
        "    df = trainDF.loc[:, trainDF.columns != 'label']\n",
        "    data = df.to_numpy().astype(np.float32)\n",
        "    kmeans = KMeans(n_clusters=num_class, random_state=0).fit(data)\n",
        "    label = kmeans.labels_\n",
        "    return label\n",
        "\n",
        "\n",
        "class DistArgs:\n",
        "    shards = None\n",
        "    requests = None\n",
        "    distribution = \"uniform\"\n",
        "    container = \"default\"\n",
        "    dataset = {}\n",
        "    label = \"latest\"\n",
        "    data = None\n",
        "    def __init__(self, dataset, data) -> None:\n",
        "        self.dataset = dataset\n",
        "        self.data = data\n",
        "\n",
        "# Load dataset metadata.\n",
        "def Distribute(args):\n",
        "    datasetfile = args.dataset\n",
        "    data = args.data\n",
        "\n",
        "    if args.shards != None:\n",
        "        # If distribution is uniform, split without optimizing.\n",
        "        if args.distribution == \"uniform\":\n",
        "            partition = np.split(\n",
        "                np.arange(0, datasetfile[\"nb_train\"]),\n",
        "                [\n",
        "                    t * (datasetfile[\"nb_train\"] // args.shards)\n",
        "                    for t in range(1, args.shards)\n",
        "                ],\n",
        "            )\n",
        "            np.save(getContainerColabRoot(args.container) + \"/splitfile.npy\", partition)\n",
        "            requests = np.array([[] for _ in range(args.shards)])\n",
        "            np.save(\n",
        "                getContainerColabRoot(args.container) + \"/requestfile:{}.npy\".format(args.label),\n",
        "                requests,\n",
        "            )\n",
        "        elif args.distribution == \"kmeans\":\n",
        "            labels = KmeansCluster(data, args.shards)\n",
        "            partition = [[] for i in range(1, args.shards)]\n",
        "            for index, row in data.iterrows():\n",
        "                label = labels.iloc[[index]]\n",
        "                label = int(label)\n",
        "                partition[label].append(index)\n",
        "            np.save(getContainerColabRoot(args.container) + \"/splitfile.npy\", partition)\n",
        "            requests = np.array([[] for _ in range(args.shards)])\n",
        "            np.save(\n",
        "                getContainerColabRoot(args.container) + \"/requestfile:{}.npy\".format(args.label),\n",
        "                requests,\n",
        "            )\n",
        "\n",
        "        elif args.distribution == \"cluster\":\n",
        "            partition = [[] for i in range(1, args.shards)]\n",
        "            for index, row in data.iterrows():\n",
        "                label = row[\"label\"]\n",
        "                label = int(label)\n",
        "                partition[label].append(index)\n",
        "            np.save(getContainerColabRoot(args.container) + \"/splitfile.npy\", partition)\n",
        "            requests = np.array([[] for _ in range(args.shards)])\n",
        "            np.save(\n",
        "                getContainerColabRoot(args.container) + \"/requestfile:{}.npy\".format(args.label),\n",
        "                requests,\n",
        "            )\n",
        "    if args.requests != None:\n",
        "        if args.distribution == \"reset\":\n",
        "            requests = np.array([[] for _ in range(partition.shape[0])])\n",
        "            np.save(\n",
        "                getContainerColabRoot(args.container) + \"/requestfile:{}.npy\".format(args.label),\n",
        "                requests,\n",
        "            )\n",
        "        else:\n",
        "            # Load splitfile.\n",
        "            partition = np.load(\n",
        "                getContainerColabRoot(args.container) + \"/splitfile.npy\", allow_pickle=True\n",
        "            )\n",
        "\n",
        "            # Randomly select points to be removed with given distribution at the dataset scale.\n",
        "            if args.distribution.split(\":\")[0] == \"exponential\":\n",
        "                lbd = (\n",
        "                    float(args.distribution.split(\":\")[1])\n",
        "                    if len(args.distribution.split(\":\")) > 1\n",
        "                    else -np.log(0.05) / datasetfile[\"nb_train\"]\n",
        "                )\n",
        "                all_requests = np.random.exponential(1 / lbd, (args.requests,))\n",
        "            if args.distribution.split(\":\")[0] == \"pareto\":\n",
        "                a = (\n",
        "                    float(args.distribution.split(\":\")[1])\n",
        "                    if len(args.distribution.split(\":\")) > 1\n",
        "                    else 1.16\n",
        "                )\n",
        "                all_requests = np.random.pareto(a, (args.requests,))\n",
        "            else:\n",
        "                all_requests = np.random.randint(0, datasetfile[\"nb_train\"], args.requests)\n",
        "\n",
        "            requests = []\n",
        "            # Divide up the new requests among the shards.\n",
        "            for shard in range(partition.shape[0]):\n",
        "                t = np.intersect1d(partition[shard], all_requests)\n",
        "                requests.append(t)\n",
        "\n",
        "            # Update requestfile.\n",
        "            np.save(\n",
        "                getContainerColabRoot(args.container) + \"/requestfile:{}.npy\".format(args.label),\n",
        "                np.array(requests, dtype=object),\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbYKJiSGichj"
      },
      "outputs": [],
      "source": [
        "args = DistArgs({\n",
        "    \"nb_train\": 60000,\n",
        "    \"nb_test\": 10000,\n",
        "    \"input_shape\": [28*28],\n",
        "    \"nb_classes\": 10,\n",
        "    \"dataloader\": dataloader\n",
        "}, traindf)\n",
        "args.shards = shards\n",
        "args.distribution = \"uniform\"\n",
        "args.container = \"\" + str(shards)\n",
        "args.label = str(0)\n",
        "Distribute(args)\n",
        "\n",
        "for j in range(NUM_REQUESTS):\n",
        "  args = DistArgs({\n",
        "    \"nb_train\": 60000,\n",
        "    \"nb_test\": 10000,\n",
        "    \"input_shape\": [28*28],\n",
        "    \"nb_classes\": 10,\n",
        "    \"dataloader\": dataloader\n",
        "  })\n",
        "  r=j*shards/5\n",
        "  args.requests = int(r)\n",
        "  args.distribution = \"kmeans\"\n",
        "  args.container = \"\" + str(shards)\n",
        "  args.label = str(int(r))\n",
        "  Distribute(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gSgdWpNuL_v",
        "outputId": "9b8f73e4-ca73-4e4c-e7de-f819de67f456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shard:  1 / 10  requests:  1 / 25\n",
            "shard:  1 / 10  requests:  2 / 25\n",
            "shard:  1 / 10  requests:  3 / 25\n",
            "shard:  1 / 10  requests:  4 / 25\n",
            "shard:  1 / 10  requests:  5 / 25\n",
            "shard:  1 / 10  requests:  6 / 25\n",
            "shard:  1 / 10  requests:  7 / 25\n",
            "shard:  1 / 10  requests:  8 / 25\n",
            "shard:  1 / 10  requests:  9 / 25\n",
            "shard:  1 / 10  requests:  10 / 25\n",
            "shard:  1 / 10  requests:  11 / 25\n",
            "shard:  1 / 10  requests:  12 / 25\n",
            "shard:  1 / 10  requests:  13 / 25\n",
            "shard:  1 / 10  requests:  14 / 25\n",
            "shard:  1 / 10  requests:  15 / 25\n",
            "shard:  1 / 10  requests:  16 / 25\n",
            "shard:  1 / 10  requests:  17 / 25\n",
            "shard:  1 / 10  requests:  18 / 25\n",
            "shard:  1 / 10  requests:  19 / 25\n",
            "shard:  1 / 10  requests:  20 / 25\n",
            "shard:  1 / 10  requests:  21 / 25\n",
            "shard:  1 / 10  requests:  22 / 25\n",
            "shard:  1 / 10  requests:  23 / 25\n",
            "shard:  1 / 10  requests:  24 / 25\n",
            "shard:  1 / 10  requests:  25 / 25\n",
            "shard:  2 / 10  requests:  1 / 25\n",
            "shard:  2 / 10  requests:  2 / 25\n",
            "shard:  2 / 10  requests:  3 / 25\n",
            "shard:  2 / 10  requests:  4 / 25\n",
            "shard:  2 / 10  requests:  5 / 25\n",
            "shard:  2 / 10  requests:  6 / 25\n",
            "shard:  2 / 10  requests:  7 / 25\n",
            "shard:  2 / 10  requests:  8 / 25\n",
            "shard:  2 / 10  requests:  9 / 25\n",
            "shard:  2 / 10  requests:  10 / 25\n",
            "shard:  2 / 10  requests:  11 / 25\n",
            "shard:  2 / 10  requests:  12 / 25\n",
            "shard:  2 / 10  requests:  13 / 25\n",
            "shard:  2 / 10  requests:  14 / 25\n",
            "shard:  2 / 10  requests:  15 / 25\n",
            "shard:  2 / 10  requests:  16 / 25\n",
            "shard:  2 / 10  requests:  17 / 25\n",
            "shard:  2 / 10  requests:  18 / 25\n",
            "shard:  2 / 10  requests:  19 / 25\n",
            "shard:  2 / 10  requests:  20 / 25\n",
            "shard:  2 / 10  requests:  21 / 25\n",
            "shard:  2 / 10  requests:  22 / 25\n",
            "shard:  2 / 10  requests:  23 / 25\n",
            "shard:  2 / 10  requests:  24 / 25\n",
            "shard:  2 / 10  requests:  25 / 25\n",
            "shard:  3 / 10  requests:  1 / 25\n",
            "shard:  3 / 10  requests:  2 / 25\n",
            "shard:  3 / 10  requests:  3 / 25\n",
            "shard:  3 / 10  requests:  4 / 25\n",
            "shard:  3 / 10  requests:  5 / 25\n",
            "shard:  3 / 10  requests:  6 / 25\n",
            "shard:  3 / 10  requests:  7 / 25\n",
            "shard:  3 / 10  requests:  8 / 25\n",
            "shard:  3 / 10  requests:  9 / 25\n",
            "shard:  3 / 10  requests:  10 / 25\n",
            "shard:  3 / 10  requests:  11 / 25\n",
            "shard:  3 / 10  requests:  12 / 25\n",
            "shard:  3 / 10  requests:  13 / 25\n",
            "shard:  3 / 10  requests:  14 / 25\n",
            "shard:  3 / 10  requests:  15 / 25\n",
            "shard:  3 / 10  requests:  16 / 25\n",
            "shard:  3 / 10  requests:  17 / 25\n",
            "shard:  3 / 10  requests:  18 / 25\n",
            "shard:  3 / 10  requests:  19 / 25\n",
            "shard:  3 / 10  requests:  20 / 25\n",
            "shard:  3 / 10  requests:  21 / 25\n",
            "shard:  3 / 10  requests:  22 / 25\n",
            "shard:  3 / 10  requests:  23 / 25\n",
            "shard:  3 / 10  requests:  24 / 25\n",
            "shard:  3 / 10  requests:  25 / 25\n",
            "shard:  4 / 10  requests:  1 / 25\n",
            "shard:  4 / 10  requests:  2 / 25\n",
            "shard:  4 / 10  requests:  3 / 25\n",
            "shard:  4 / 10  requests:  4 / 25\n",
            "shard:  4 / 10  requests:  5 / 25\n",
            "shard:  4 / 10  requests:  6 / 25\n",
            "shard:  4 / 10  requests:  7 / 25\n",
            "shard:  4 / 10  requests:  8 / 25\n",
            "shard:  4 / 10  requests:  9 / 25\n",
            "shard:  4 / 10  requests:  10 / 25\n",
            "shard:  4 / 10  requests:  11 / 25\n",
            "shard:  4 / 10  requests:  12 / 25\n",
            "shard:  4 / 10  requests:  13 / 25\n",
            "shard:  4 / 10  requests:  14 / 25\n",
            "shard:  4 / 10  requests:  15 / 25\n",
            "shard:  4 / 10  requests:  16 / 25\n",
            "shard:  4 / 10  requests:  17 / 25\n",
            "shard:  4 / 10  requests:  18 / 25\n",
            "shard:  4 / 10  requests:  19 / 25\n",
            "shard:  4 / 10  requests:  20 / 25\n",
            "shard:  4 / 10  requests:  21 / 25\n",
            "shard:  4 / 10  requests:  22 / 25\n",
            "shard:  4 / 10  requests:  23 / 25\n",
            "shard:  4 / 10  requests:  24 / 25\n",
            "shard:  4 / 10  requests:  25 / 25\n",
            "shard:  5 / 10  requests:  1 / 25\n",
            "shard:  5 / 10  requests:  2 / 25\n",
            "shard:  5 / 10  requests:  3 / 25\n",
            "shard:  5 / 10  requests:  4 / 25\n",
            "shard:  5 / 10  requests:  5 / 25\n",
            "shard:  5 / 10  requests:  6 / 25\n",
            "shard:  5 / 10  requests:  7 / 25\n",
            "shard:  5 / 10  requests:  8 / 25\n",
            "shard:  5 / 10  requests:  9 / 25\n",
            "shard:  5 / 10  requests:  10 / 25\n",
            "shard:  5 / 10  requests:  11 / 25\n",
            "shard:  5 / 10  requests:  12 / 25\n",
            "shard:  5 / 10  requests:  13 / 25\n",
            "shard:  5 / 10  requests:  14 / 25\n",
            "shard:  5 / 10  requests:  15 / 25\n",
            "shard:  5 / 10  requests:  16 / 25\n",
            "shard:  5 / 10  requests:  17 / 25\n",
            "shard:  5 / 10  requests:  18 / 25\n",
            "shard:  5 / 10  requests:  19 / 25\n",
            "shard:  5 / 10  requests:  20 / 25\n",
            "shard:  5 / 10  requests:  21 / 25\n",
            "shard:  5 / 10  requests:  22 / 25\n",
            "shard:  5 / 10  requests:  23 / 25\n",
            "shard:  5 / 10  requests:  24 / 25\n",
            "shard:  5 / 10  requests:  25 / 25\n",
            "shard:  6 / 10  requests:  1 / 25\n",
            "shard:  6 / 10  requests:  2 / 25\n",
            "shard:  6 / 10  requests:  3 / 25\n",
            "shard:  6 / 10  requests:  4 / 25\n",
            "shard:  6 / 10  requests:  5 / 25\n",
            "shard:  6 / 10  requests:  6 / 25\n",
            "shard:  6 / 10  requests:  7 / 25\n",
            "shard:  6 / 10  requests:  8 / 25\n",
            "shard:  6 / 10  requests:  9 / 25\n",
            "shard:  6 / 10  requests:  10 / 25\n",
            "shard:  6 / 10  requests:  11 / 25\n",
            "shard:  6 / 10  requests:  12 / 25\n",
            "shard:  6 / 10  requests:  13 / 25\n",
            "shard:  6 / 10  requests:  14 / 25\n",
            "shard:  6 / 10  requests:  15 / 25\n",
            "shard:  6 / 10  requests:  16 / 25\n",
            "shard:  6 / 10  requests:  17 / 25\n",
            "shard:  6 / 10  requests:  18 / 25\n",
            "shard:  6 / 10  requests:  19 / 25\n",
            "shard:  6 / 10  requests:  20 / 25\n",
            "shard:  6 / 10  requests:  21 / 25\n",
            "shard:  6 / 10  requests:  22 / 25\n",
            "shard:  6 / 10  requests:  23 / 25\n",
            "shard:  6 / 10  requests:  24 / 25\n",
            "shard:  6 / 10  requests:  25 / 25\n",
            "shard:  7 / 10  requests:  1 / 25\n",
            "shard:  7 / 10  requests:  2 / 25\n",
            "shard:  7 / 10  requests:  3 / 25\n",
            "shard:  7 / 10  requests:  4 / 25\n",
            "shard:  7 / 10  requests:  5 / 25\n",
            "shard:  7 / 10  requests:  6 / 25\n",
            "shard:  7 / 10  requests:  7 / 25\n",
            "shard:  7 / 10  requests:  8 / 25\n",
            "shard:  7 / 10  requests:  9 / 25\n",
            "shard:  7 / 10  requests:  10 / 25\n",
            "shard:  7 / 10  requests:  11 / 25\n",
            "shard:  7 / 10  requests:  12 / 25\n",
            "shard:  7 / 10  requests:  13 / 25\n",
            "shard:  7 / 10  requests:  14 / 25\n",
            "shard:  7 / 10  requests:  15 / 25\n",
            "shard:  7 / 10  requests:  16 / 25\n",
            "shard:  7 / 10  requests:  17 / 25\n",
            "shard:  7 / 10  requests:  18 / 25\n",
            "shard:  7 / 10  requests:  19 / 25\n",
            "shard:  7 / 10  requests:  20 / 25\n",
            "shard:  7 / 10  requests:  21 / 25\n",
            "shard:  7 / 10  requests:  22 / 25\n",
            "shard:  7 / 10  requests:  23 / 25\n",
            "shard:  7 / 10  requests:  24 / 25\n",
            "shard:  7 / 10  requests:  25 / 25\n",
            "shard:  8 / 10  requests:  1 / 25\n",
            "shard:  8 / 10  requests:  2 / 25\n",
            "shard:  8 / 10  requests:  3 / 25\n",
            "shard:  8 / 10  requests:  4 / 25\n",
            "shard:  8 / 10  requests:  5 / 25\n",
            "shard:  8 / 10  requests:  6 / 25\n",
            "shard:  8 / 10  requests:  7 / 25\n",
            "shard:  8 / 10  requests:  8 / 25\n",
            "shard:  8 / 10  requests:  9 / 25\n",
            "shard:  8 / 10  requests:  10 / 25\n",
            "shard:  8 / 10  requests:  11 / 25\n",
            "shard:  8 / 10  requests:  12 / 25\n",
            "shard:  8 / 10  requests:  13 / 25\n",
            "shard:  8 / 10  requests:  14 / 25\n",
            "shard:  8 / 10  requests:  15 / 25\n",
            "shard:  8 / 10  requests:  16 / 25\n",
            "shard:  8 / 10  requests:  17 / 25\n",
            "shard:  8 / 10  requests:  18 / 25\n",
            "shard:  8 / 10  requests:  19 / 25\n",
            "shard:  8 / 10  requests:  20 / 25\n",
            "shard:  8 / 10  requests:  21 / 25\n",
            "shard:  8 / 10  requests:  22 / 25\n",
            "shard:  8 / 10  requests:  23 / 25\n",
            "shard:  8 / 10  requests:  24 / 25\n",
            "shard:  8 / 10  requests:  25 / 25\n",
            "shard:  9 / 10  requests:  1 / 25\n",
            "shard:  9 / 10  requests:  2 / 25\n",
            "shard:  9 / 10  requests:  3 / 25\n",
            "shard:  9 / 10  requests:  4 / 25\n",
            "shard:  9 / 10  requests:  5 / 25\n",
            "shard:  9 / 10  requests:  6 / 25\n",
            "shard:  9 / 10  requests:  7 / 25\n",
            "shard:  9 / 10  requests:  8 / 25\n",
            "shard:  9 / 10  requests:  9 / 25\n",
            "shard:  9 / 10  requests:  10 / 25\n",
            "shard:  9 / 10  requests:  11 / 25\n",
            "shard:  9 / 10  requests:  12 / 25\n",
            "shard:  9 / 10  requests:  13 / 25\n",
            "shard:  9 / 10  requests:  14 / 25\n",
            "shard:  9 / 10  requests:  15 / 25\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "args = SisaArgs(None, {\n",
        "    \"nb_train\": 60000,\n",
        "    \"nb_test\": 10000,\n",
        "    \"input_shape\": [28*28],\n",
        "    \"nb_classes\": 10,\n",
        "    \"dataloader\": dataloader\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "for i in range(shards):\n",
        "  for j in range(NUM_REQUESTS):\n",
        "    print(\"shard: \", i+1, \"/\", shards, \" requests: \", j+1, \"/\", NUM_REQUESTS)\n",
        "    r=j*shards/5\n",
        "    args.train = True\n",
        "    args.slices = 1\n",
        "    args.label = str(int(r))\n",
        "    args.epochs = 20\n",
        "    args.batch_size = 16\n",
        "    args.chkpt_interval = 1\n",
        "    args.container = \"\" + str(shards)\n",
        "    args.shard = np.int64(i)\n",
        "    args.model = Model\n",
        "    sisa(args)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU9w9KMZ12QP"
      },
      "outputs": [],
      "source": [
        "args = SisaArgs(None, {\n",
        "    \"nb_train\": 60000,\n",
        "    \"nb_test\": 10000,\n",
        "    \"input_shape\": [28*28],\n",
        "    \"nb_classes\": 10,\n",
        "    \"dataloader\": dataloader\n",
        "})\n",
        "\n",
        "for i in range(shards):\n",
        "  for j in range(NUM_REQUESTS):\n",
        "    print(\"shard: \", i+1, \"/\", shards, \" requests: \", j+1, \"/16\")\n",
        "    r=j*shards/5\n",
        "    args.test = True\n",
        "    args.label = str(int(r))\n",
        "    args.batch_size = 16\n",
        "    args.container = \"\" + str(shards)\n",
        "    args.shard = np.int64(i)\n",
        "    args.model = Model\n",
        "    sisa(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhytV2-jVLrK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class AggrArgs:\n",
        "    strategy = \"uniform\"\n",
        "    container = \"\"\n",
        "    shards = 1\n",
        "    dataset = {}\n",
        "    baseline = None\n",
        "    label = \"latest\"\n",
        "    dataset = None\n",
        "\n",
        "    def __init__(self, dataset) -> None:\n",
        "        self.dataset = dataset\n",
        "\n",
        "def Aggregate(args):\n",
        "    # Load dataset metadata.\n",
        "    datasetfile = args.dataset\n",
        "    dataloader = args.dataset[\"dataloader\"]\n",
        "\n",
        "    # Output files used for the vote.\n",
        "    if args.baseline != None:\n",
        "        filenames = [\"shard-{}:{}.npy\".format(args.baseline, args.label)]\n",
        "    else:\n",
        "        filenames = [\"shard-{}:{}.npy\".format(i, args.label) for i in range(args.shards)]\n",
        "\n",
        "    # Concatenate output files.\n",
        "    outputs = []\n",
        "    for filename in filenames:\n",
        "        outputs.append(\n",
        "            np.load(\n",
        "                getOutputs(args.container) + \"/\" + filename,\n",
        "                allow_pickle=True,\n",
        "            )\n",
        "        )\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    # Compute weight vector based on given strategy.\n",
        "    if args.strategy == \"uniform\":\n",
        "        weights = (\n",
        "            1 / outputs.shape[0] * np.ones((outputs.shape[0],))\n",
        "        )  # pylint: disable=unsubscriptable-object\n",
        "    elif args.strategy.startswith(\"models:\"):\n",
        "        models = np.array(args.strategy.split(\":\")[1].split(\",\")).astype(int)\n",
        "        weights = np.zeros((outputs.shape[0],))  # pylint: disable=unsubscriptable-object\n",
        "        weights[models] = 1 / models.shape[0]  # pylint: disable=unsubscriptable-object\n",
        "    elif args.strategy == \"proportional\":\n",
        "        split = np.load(\n",
        "            getContainerColabRoot(args.container) + \"/splitfile.npy\"\n",
        "            , allow_pickle=True\n",
        "        )\n",
        "        weights = np.array([shard.shape[0] for shard in split])\n",
        "\n",
        "    # Tensor contraction of outputs and weights (on the shard dimension).\n",
        "    votes = np.argmax(\n",
        "        np.tensordot(weights.reshape(1, weights.shape[0]), outputs, axes=1), axis=2\n",
        "    ).reshape(\n",
        "        (outputs.shape[1],)\n",
        "    )  # pylint: disable=unsubscriptable-object\n",
        "\n",
        "    # Load labels.\n",
        "    _, labels = dataloader(np.arange(datasetfile[\"nb_test\"]), category=\"test\")\n",
        "\n",
        "    # Compute and print accuracy.\n",
        "    accuracy = (\n",
        "        np.where(votes == labels)[0].shape[0] / outputs.shape[1]\n",
        "    )  # pylint: disable=unsubscriptable-object\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-uCr90iXuT8"
      },
      "outputs": [],
      "source": [
        "args = AggrArgs({\n",
        "    \"nb_train\": 60000,\n",
        "    \"nb_test\": 10000,\n",
        "    \"input_shape\": [28*28],\n",
        "    \"nb_classes\": 10,\n",
        "    \"dataloader\": dataloader\n",
        "})\n",
        "\n",
        "stats = []\n",
        "for j in range(NUM_REQUESTS):\n",
        "  r=j*shards/5\n",
        "  args.strategy = \"uniform\"\n",
        "  args.container = \"\" + str(shards)\n",
        "  args.shards = shards\n",
        "  args.label = str(int(r))\n",
        "  acc = Aggregate(args)\n",
        "  stats.append(acc)\n",
        "  print(\"shards: \", shards, \"r:\", r, \"acc: \", acc)\n",
        "\n",
        "print(stats)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
